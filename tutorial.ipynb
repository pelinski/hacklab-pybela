{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybela tutorial\n",
    "In this workshop we'll be using jupyter notebooks and python to:\n",
    "1. Record a dataset of potentiometer values\n",
    "2. Train an RNN to predict the potentiometer's values\n",
    "3. Cross-compile and deploy the model to run in real-time in Bela\n",
    "\n",
    "First, we need to copy the dataset capturing code into Bela. Connect the Bela to your laptop, wait for a few seconds so that the connection is established, and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh-keyscan $BBB_HOSTNAME >> ~/.ssh/known_hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -rvL bela-code/dataset-capture root@$BBB_HOSTNAME:Bela/projects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Collect dataset\n",
    "We will record a dataset of potentiometer movements. \n",
    "- Connect the left and right pins of the potentiometer to the ground and 3.3V pins in Bela and the middle pin to the analog input A0.\n",
    "- Run the `dataset-capture` project on Bela (you can do so from the IDE)\n",
    "- Connect an aux cable to your phone and play a song, and plug it into the Bela input. Connect your headphones to the Bela output.\n",
    "\n",
    "The potentiometer controls the shape of an LFO applied to the input audio signal. Play a bit with the potentiometer, and when you are ready for a 1-2min performance, run the cell below to start recording a dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybela import Logger\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "logger=Logger(ip=os.environ[\"BBB_HOSTNAME\"])\n",
    "logger.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = logger.start_logging(variables=[\"pot\"])\n",
    "await asyncio.sleep(10)\n",
    "logger.stop_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ~1-2 min, you can stop recording the dataset by running the next cell. You can also record for longer if you prefer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = logger.read_binary_file(\n",
    "        file_path=file_paths[\"local_paths\"][\"pot\"], timestamp_mode=logger.get_prop_of_var(\"pot\", \"timestamp_mode\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Train model\n",
    "Now we are ready to train our model. First, import the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the dataset using the streamer method `load_data_from_file()`. We can generate a pytorch compatible dataset using the `PotentiometerDataset` class. This class divides the data you recorded previously in sequences of 32 values. A sequence of 32 values will be passed to the network, which will predict the next sequence of 32 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data for _buffer in raw[\"buffers\"] for data in _buffer[\"data\"]]\n",
    "\n",
    "seq_len = 32\n",
    "batch_size = 64\n",
    "\n",
    "class PotentiometerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # make len divisible by seq_len\n",
    "        data = data[:len(data) - (len(data) % seq_len)]\n",
    "        sequences = [data[i:i+seq_len] for i in range(0, len(data), seq_len)]\n",
    "\n",
    "        self.inputs = torch.tensor(sequences[:-1]).float().to(self.device)\n",
    "        self.outputs = torch.tensor(sequences[1:]).float().to(self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.inputs[i].unsqueeze(dim=1), self.outputs[i].unsqueeze(dim=1)\n",
    "    \n",
    "dataset = PotentiometerDataset(data, seq_len)\n",
    "\n",
    "# Split dataset\n",
    "train_count = int(0.9 * dataset.__len__())\n",
    "test_count = dataset.__len__() - train_count\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, (train_count, test_count)\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a simple RNN with a hidden size of 64. We will use an SGD optimiser with a learning rate of 0.001 and use the mean square error as loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Apply the linear layer to get the final output\n",
    "        out = self.fc(out)\n",
    "   \n",
    "        \n",
    "        return out\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(input_size=1, hidden_size=64, output_size=1).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "print(\"Running on device: {}\".format(device))\n",
    "for epoch in range(1, epochs+1):\n",
    "\n",
    "    print(\"█▓░ Epoch: {} ░▓█\".format(epoch))\n",
    "\n",
    "    # training loop\n",
    "    train_it_losses = np.array([])\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # (batch_size, seq_len, input_size)\n",
    "        data = data.to(device=device, non_blocking=True)\n",
    "        # (batch_size, seq_len, input_size)\n",
    "        targets = targets.to(device=device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # lower memory footprint\n",
    "        out = model(data)\n",
    "        train_loss = torch.sqrt(criterion(out, targets))\n",
    "        train_it_losses = np.append(train_it_losses, train_loss.item())\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # test loop\n",
    "    test_it_losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(test_loader)):\n",
    "        # (batch_size, seq_length, input_size)\n",
    "        data = data.to(device=device, non_blocking=True)\n",
    "        # (batch_size, seq_length, out_size)\n",
    "        targets = targets.to(device=device, non_blocking=True)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)  # using predict method to avoid backprop\n",
    "        test_loss = torch.sqrt(criterion(out, targets))\n",
    "        test_it_losses = np.append(\n",
    "            test_it_losses, test_loss.item())\n",
    "\n",
    "    losses = {\"train_loss\": train_it_losses.mean().round(\n",
    "        8), \"test_loss\": test_it_losses.mean().round(8)}\n",
    "    pp.pprint(losses, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If you get a `RuntimeError: could not create a primitive descriptor for a matmul primitive` error here --> check the `readme-silicon.md`. This error seems to happen when training on a jupyter notebook running on a docker container on Mac M1/M2. In the `readme-silicon.md` there are instructions for running the notebook locally (in your laptop, not in the container) so that this error doesn't appear.\n",
    "\n",
    "Let's make sure the model trained correctly by visualising some of the predictions in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random indexes for plotting\n",
    "num_figures = 8\n",
    "random_indexes = np.random.choice(len(test_dataset), size=num_figures, replace=False)\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "num_rows = num_figures // 2\n",
    "num_cols = 2\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 3 * num_rows))\n",
    "\n",
    "# Flatten the axes array to simplify iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through random indexes and plot predictions\n",
    "for idx, ax in zip(random_indexes, axes):\n",
    "    input, target = test_dataset.__getitem__(idx)\n",
    "    output = model(input.unsqueeze(0))\n",
    "    \n",
    "    ax.plot(target.view(-1).detach().cpu(), label='Target')\n",
    "    ax.plot(output.view(-1).detach().cpu(), label='Predictions')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_title(f'Figure for Index {idx}')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for i in range(len(random_indexes), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're ready, save the model so that we can export it into Bela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device='cpu')\n",
    "model.eval()\n",
    "script = torch.jit.script(model)\n",
    "path = \"bela-code/pot-inference/model.jit\"\n",
    "script.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.load(path) # check model is properly saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Deploy and run\n",
    "\n",
    "The cell below will cross-compile and deploy the project to Bela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd bela-code/pot-inference/ && sh build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed, you can run it from the Bela terminal (which you can access from your regular terminal typing `ssh root@bela.local`) by typing:\n",
    "```bash\n",
    "cd Bela/projects/pot-inference\n",
    "./pot-inference --modelPath model.jit\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
